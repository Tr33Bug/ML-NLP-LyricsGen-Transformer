{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba81caa1",
   "metadata": {},
   "source": [
    "# GPT2 Training Loop\n",
    "\n",
    "Notebook to export the test data for evaluation and train the dataset on the GPT2 model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52787d01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation: \n",
    "# ! pip install pandas\n",
    "# ! pip install datasets\n",
    "# ! pip install transformers\n",
    "# ! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58d3fc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "⏳⏳⏳ Starting GPT2 TRAINING by Overfitter ⏳⏳⏳\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('⏳⏳⏳ Starting GPT2 TRAINING by Overfitter ⏳⏳⏳')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26679df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee517886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ --> All imports are done!\n",
      " --> FILE NAME:  /Users/tree/miniconda3/envs/Ki-Lab3.9/lib/python3.9/site-packages/ipykernel_launcher.py\n"
     ]
    }
   ],
   "source": [
    "print('✅ --> All imports are done!')\n",
    "\n",
    "# get own file name\n",
    "import sys\n",
    "file_name = sys.argv[0]\n",
    "\n",
    "print(' --> FILE NAME: ', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a8f33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Set paths and parametes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4fcda6",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "591dbcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dataset\n",
    "\n",
    "trainData = 'both' # 'rap' 'both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baff25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for randaomness\n",
    "seed = 0\n",
    "\n",
    "#Seeds and hyperparameters\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# set random seed\n",
    "random.seed(seed)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_STEPS = 200\n",
    "EPSILION = 1e-8\n",
    "\n",
    "# this produces sample output every 100 steps\n",
    "sample_every = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2876f",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e3dcac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh say can you see by the dawns early light\\nW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last Saturday night I got married\\nMe and my w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The sun is out\\nThe sky is blue\\nTheres not a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It barks at no one else but me\\nLike its seen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ride Into the Sun Session Outtake Lyrics\\nLook...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Lyrics\n",
       "0  Oh say can you see by the dawns early light\\nW...\n",
       "1  Last Saturday night I got married\\nMe and my w...\n",
       "2  The sun is out\\nThe sky is blue\\nTheres not a ...\n",
       "3  It barks at no one else but me\\nLike its seen ...\n",
       "4  Ride Into the Sun Session Outtake Lyrics\\nLook..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "\n",
    "# top dataset\n",
    "df_top = pd.read_csv('./datasets/df_top.csv')\n",
    "df_top  = df_top.drop(columns=['Song', 'Artist'])\n",
    "\n",
    "# rap dataset\n",
    "df_rap = pd.read_csv('./datasets/df_rap.csv')\n",
    "df_rap  = df_rap.drop(columns=['Song', 'Artist'])\n",
    "\n",
    "# topRap merge dataset\n",
    "df_songs = pd.read_csv('./datasets/df_songs.csv')\n",
    "df_songs = df_songs.drop(columns=['Song','LyricsWordCount', 'Artist'])#\n",
    "\n",
    "# chose dataset on trainData string\n",
    "if trainData == 'top':\n",
    "    lyrics_df = df_top\n",
    "elif trainData == 'rap':\n",
    "    lyrics_df = df_rap\n",
    "elif trainData == 'both':\n",
    "    lyrics_df = df_songs\n",
    "else:\n",
    "    print('❌ --> ERROR: trainData string is not correct')\n",
    "\n",
    "\n",
    "\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "758502d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ --> Loaded dataset:  both\n"
     ]
    }
   ],
   "source": [
    "print('✅ --> Loaded dataset: ', trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370f04c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d30fca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(lyrics_df) * 0.10)\n",
    "test_samples = lyrics_df.sample(n, random_state=0)\n",
    "lyrics_df = lyrics_df.drop(test_samples.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "928163af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples[\"True_end_lyrics\"] = \"\"\n",
    "test_samples[\"Lyrics_Cut\"] = \"\"\n",
    "\n",
    "for row in test_samples.iterrows():\n",
    "    lyrics = str(row[1]['Lyrics'])\n",
    "    lyrics = lyrics.split()\n",
    "    split = int(len(lyrics) * 0.5)\n",
    "    \n",
    "    lyrics_cut = lyrics[:split]\n",
    "    true_end_lyrics = lyrics[split:]\n",
    "    \n",
    "    true_end_lyrics = \" \".join(true_end_lyrics)\n",
    "    lyrics_cut = \" \".join(lyrics_cut)\n",
    "    \n",
    "    row[1][\"True_end_lyrics\"] = true_end_lyrics\n",
    "    row[1][\"Lyrics_Cut\"] = lyrics_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72c134cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ --> Exported test sample dataset to :  ./datasets/both_test_samples.csv\n"
     ]
    }
   ],
   "source": [
    "path_test_samples = './datasets/' + trainData + \"_test_samples.csv\"\n",
    "test_samples.to_csv(path_test_samples)\n",
    "print('✅ --> Exported test sample dataset to : ', path_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ede07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27798f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf934d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + str(txt) + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1babcca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,285 training samples\n",
      "  365 validation samples\n"
     ]
    }
   ],
   "source": [
    "lyricList = lyrics_df[\"Lyrics\"].tolist()\n",
    "dataset = GPT2Dataset(lyricList, tokenizer, max_length=768)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23bf77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = BATCH_SIZE # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = BATCH_SIZE # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b602d1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE, eps = EPSILION)\n",
    "\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = WARMUP_STEPS, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b14acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d71f69",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('⏳ --> Start training-loop!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abfbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_t0 = time.time()\n",
    "training_stats = []\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    model = model.cuda()\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "    print('Training...\\n')\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    print('✅ --> Epoche', epoch_i, 'model.train done!\\n')\n",
    "\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids,labels=b_labels, attention_mask = b_masks, token_type_ids=None)\n",
    "        loss = outputs[0]  \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % sample_every == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('✅ Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.\\n'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "            print('⏳ --> Start evaluating model!\\n')\n",
    "            model.eval()\n",
    "            print('✅ --> Done evaluating model!\\n')\n",
    "            sample_outputs = model.generate(\n",
    "                                    bos_token_id=random.randint(1,30000),\n",
    "                                    do_sample=True,   \n",
    "                                    top_k=50, \n",
    "                                    max_length = 200,\n",
    "                                    top_p=0.95, \n",
    "                                    num_return_sequences=1\n",
    "                                )\n",
    "            print('✅ --> Model evaluation done!\\n')\n",
    "            print('⏳ --> Print test-generated text!\\n')\n",
    "            for i, sample_output in enumerate(sample_outputs):\n",
    "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "            \n",
    "            print('✅ --> Print test-generated text done!\\n')\n",
    "            \n",
    "            print('⏳ --> Start training model again, in sample Loop\\n')\n",
    "            model.train()\n",
    "            print('✅ --> Done training model again, in sample Loop\\n')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)       \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"✅ --> Training epoch done!\\n\")\n",
    "    print(\"Average training loss: {0:.2f}\\n\".format(avg_train_loss))\n",
    "    print(\"Training epoch took: {:}\\n\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print('⏳ --> Start validating model!\\n')\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs  = model(b_input_ids, \n",
    "#                            token_type_ids=None, \n",
    "                             attention_mask = b_masks,\n",
    "                            labels=b_labels)\n",
    "          \n",
    "            loss = outputs[0]  \n",
    "            \n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss        \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)    \n",
    "\n",
    "    print(\"✅ --> Validation epoch done!\\n\")\n",
    "    print(\"Validation Loss: {0:.2f}\\n\".format(avg_val_loss))\n",
    "    print(\"Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"✅ Training complete!\\n\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "print('⏳ --> Start saving model!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ac8ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Saving training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2895fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "output_dir = './model_save/' + trainData + '/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('✅ --> Model saved!\\n')\n",
    "print('⏳ --> Start saving training stats!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d39315",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('precision', 2)\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# export dataframe to csv\n",
    "df_stats.to_csv('training_stats_' + trainData + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('✅ --> Training stats saved!\\n')\n",
    "\n",
    "print('======================================== TRAINING DONE ========================================')\n",
    "# print date and time\n",
    "print('Date and time:', datetime.datetime.now())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "307e109b",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "401949f8f22344ab70dfcd116b53776ca8bac82eaa601e9464a5719ba1fa738c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
