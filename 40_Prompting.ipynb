{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EI4nnQRJVZS"
   },
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww61aS9EJVZU"
   },
   "source": [
    "One of the few resources found to Prefix Templates with OpenPrompt\n",
    "\n",
    "> https://github.com/thunlp/OpenPrompt/blob/main/tutorial/2.1_conditional_generation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egcG83X7JVZU"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:00.883845Z",
     "iopub.status.busy": "2022-12-08T09:49:00.883397Z",
     "iopub.status.idle": "2022-12-08T09:49:04.506619Z",
     "shell.execute_reply": "2022-12-08T09:49:04.505438Z",
     "shell.execute_reply.started": "2022-12-08T09:49:00.883771Z"
    },
    "id": "ffM9qE-KJVZU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q openprompt transformers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:04.509290Z",
     "iopub.status.busy": "2022-12-08T09:49:04.508948Z",
     "iopub.status.idle": "2022-12-08T09:49:09.398037Z",
     "shell.execute_reply": "2022-12-08T09:49:09.396891Z",
     "shell.execute_reply.started": "2022-12-08T09:49:04.509249Z"
    },
    "id": "Zbaz3p53JVZV",
    "outputId": "f8f4c51b-b7ce-403d-f573-3e09d891ab6e"
   },
   "outputs": [],
   "source": [
    "from openprompt import PromptDataLoader, PromptForGeneration\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import PrefixTuningTemplate\n",
    "from openprompt.utils.metrics import generation_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets.dataset_dict import DatasetDict, Dataset\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:09.399629Z",
     "iopub.status.busy": "2022-12-08T09:49:09.399185Z",
     "iopub.status.idle": "2022-12-08T09:49:09.404658Z",
     "shell.execute_reply": "2022-12-08T09:49:09.403701Z",
     "shell.execute_reply.started": "2022-12-08T09:49:09.399602Z"
    },
    "id": "dpUqLER2JVZV",
    "outputId": "5eabfcde-b5d0-406c-9fcc-32761d026c7b"
   },
   "outputs": [],
   "source": [
    "\n",
    "base_path = \"./evaluations/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a642EBPoJVZW"
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:09.406661Z",
     "iopub.status.busy": "2022-12-08T09:49:09.406303Z",
     "iopub.status.idle": "2022-12-08T09:49:09.411378Z",
     "shell.execute_reply": "2022-12-08T09:49:09.410626Z",
     "shell.execute_reply.started": "2022-12-08T09:49:09.406634Z"
    },
    "id": "cG0Q3USeJVZW"
   },
   "outputs": [],
   "source": [
    "csv_data = \"./df_songs.csv\"\n",
    "used_model = \"gpt2\"\n",
    "\n",
    "train_split = 0.7\n",
    "epochs = 5\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBbBxjgJJVZW"
   },
   "source": [
    "### Model / Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgSAYI38JVZW"
   },
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYmGpwM_JVZW"
   },
   "source": [
    "Read the CSV, remove everything except the lyrics. Add index for flavour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:09.414440Z",
     "iopub.status.busy": "2022-12-08T09:49:09.414069Z",
     "iopub.status.idle": "2022-12-08T09:49:09.827545Z",
     "shell.execute_reply": "2022-12-08T09:49:09.826551Z",
     "shell.execute_reply.started": "2022-12-08T09:49:09.414413Z"
    },
    "id": "23XO29bcJVZX"
   },
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv(csv_data)\n",
    "lyrics_df = lyrics_df.drop(\n",
    "    columns=[\"Artist\", \"Song\", \"LyricsWordCount\"], errors=\"ignore\"\n",
    ").reset_index(level=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZYcEisiJVZX"
   },
   "source": [
    "Split the dataset and create an DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:09.829081Z",
     "iopub.status.busy": "2022-12-08T09:49:09.828827Z",
     "iopub.status.idle": "2022-12-08T09:49:09.876836Z",
     "shell.execute_reply": "2022-12-08T09:49:09.875667Z",
     "shell.execute_reply.started": "2022-12-08T09:49:09.829043Z"
    },
    "id": "H89FlH7YJVZX"
   },
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(lyrics_df, train_size=train_split)\n",
    "train_dataset, validation_dataset = Dataset.from_pandas(train_df), Dataset.from_pandas(\n",
    "    validation_df\n",
    ")\n",
    "raw_dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8I8ZAinJVZX"
   },
   "source": [
    "Create a new dataset with a mapped `InputExample` for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:09.878739Z",
     "iopub.status.busy": "2022-12-08T09:49:09.878051Z",
     "iopub.status.idle": "2022-12-08T09:49:10.595816Z",
     "shell.execute_reply": "2022-12-08T09:49:10.594870Z",
     "shell.execute_reply.started": "2022-12-08T09:49:09.878709Z"
    },
    "id": "s1mC1QKlJVZX"
   },
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    for data in raw_dataset[split]:\n",
    "        # input_example = InputExample(text_a = data['premise'], text_b = data['hypothesis'], label=int(data['label']), guid=data['idx'])\n",
    "        input_example = InputExample(text_a = data['Lyrics'], guid=data['index'])\n",
    "        dataset[split].append(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r15nV3nHJVZY"
   },
   "source": [
    "Steal this dataloader wrapper function üê±‚Äçüë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:10.597575Z",
     "iopub.status.busy": "2022-12-08T09:49:10.597575Z",
     "iopub.status.idle": "2022-12-08T09:49:10.603912Z",
     "shell.execute_reply": "2022-12-08T09:49:10.602505Z",
     "shell.execute_reply.started": "2022-12-08T09:49:10.597575Z"
    },
    "id": "DjkiBTREJVZY"
   },
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    dataset_split, template, tokenizer, wrapper_class, shuffle=False, batch_size=32\n",
    "):\n",
    "    \"\"\"Returns a prompt data load for a given dataset split and template\"\"\"\n",
    "\n",
    "    return PromptDataLoader(\n",
    "        dataset=dataset_split,\n",
    "        template=template,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=wrapper_class,\n",
    "        max_seq_length=256,\n",
    "        decoder_max_length=256,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        teacher_forcing=False,\n",
    "        predict_eos_token=True,\n",
    "        truncate_method=\"head\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ihqV0coJVZY"
   },
   "source": [
    "#### Model (PLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:10.606332Z",
     "iopub.status.busy": "2022-12-08T09:49:10.605738Z",
     "iopub.status.idle": "2022-12-08T09:49:14.106152Z",
     "shell.execute_reply": "2022-12-08T09:49:14.105305Z",
     "shell.execute_reply.started": "2022-12-08T09:49:10.606020Z"
    },
    "id": "jPB017P_JVZY",
    "outputId": "fce75bc2-ea3b-4796-eca0-c829d0d6eb1d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm(used_model, used_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:14.107838Z",
     "iopub.status.busy": "2022-12-08T09:49:14.107598Z",
     "iopub.status.idle": "2022-12-08T09:49:14.111768Z",
     "shell.execute_reply": "2022-12-08T09:49:14.110838Z",
     "shell.execute_reply.started": "2022-12-08T09:49:14.107815Z"
    },
    "id": "hk5O_FrfJVZZ"
   },
   "outputs": [],
   "source": [
    "# # tokenizer = GPT2Tokenizer.from_pretrained(used_model, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "\n",
    "# tokenizer.bos_token = \"<|startoftext|>\"\n",
    "# tokenizer.eos_token = \"<|endoftext|>\"\n",
    "# tokenizer.pad_token = \"<|pad|>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uTbv2NUJVZa"
   },
   "source": [
    "Steal this evaluate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCCiUqLVJVZa"
   },
   "source": [
    "### Prompt-Based Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKY5CHuZJVZa"
   },
   "source": [
    "Create a template.\n",
    "The used template (line 1) equals the last template (line 7), so that the text param can be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:14.113374Z",
     "iopub.status.busy": "2022-12-08T09:49:14.113138Z",
     "iopub.status.idle": "2022-12-08T09:49:14.293754Z",
     "shell.execute_reply": "2022-12-08T09:49:14.292367Z",
     "shell.execute_reply.started": "2022-12-08T09:49:14.113373Z"
    },
    "id": "XFW8P0mQJVZb",
    "outputId": "7b618ce2-3c04-43d0-dd91-18b5e5576a1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignore using_decoder_past_key_values=False in a decoder-only LM.\n"
     ]
    }
   ],
   "source": [
    "template = PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text=' {\"placeholder\":\"text_a\"} {\"special\": \"<eos>\"} {\"mask\"} ', using_decoder_past_key_values=False)\n",
    "\n",
    "# Are the tokens necessary? Probably not:\n",
    "# # You may observe that the example doesn't end with <|endoftext|> token. Don't worry, adding specific end-of-text token\n",
    "# # is a language-model-specific token. we will add it for you in the TokenizerWrapper once you pass `predict_eos_token=True`\n",
    "\n",
    "# template = PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text='<|startoftext|>{\"placeholder\":\"text_a\"} {\"mask\"}<|endoftext|>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVv3j4VPJVZb"
   },
   "source": [
    "Create one example and print it, to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:14.295626Z",
     "iopub.status.busy": "2022-12-08T09:49:14.295325Z",
     "iopub.status.idle": "2022-12-08T09:49:14.302125Z",
     "shell.execute_reply": "2022-12-08T09:49:14.300797Z",
     "shell.execute_reply.started": "2022-12-08T09:49:14.295602Z"
    },
    "id": "4Z7IzKnnJVZc",
    "outputId": "a8c8dd01-b30b-4e2f-d6a6-df4bc36a9c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': ' I spell BAD\\nIm bad\\nThat means no messing me\\nNow all you women\\nRemember when I was twentyone\\nThat was a year baby\\nThat I had alots of fun\\nBut a year has gone by\\nIm now twentytwo\\nI can eat nails honey\\nAnd drink gunpowder soup\\nI spell BAD\\nIm bad\\nThat means no messing me\\nNow honey you better tell your husband\\nQuit sneaking peeping at me\\nHe wanna fight off the man\\nThat Im the supposed to be\\nIll make sure the drills running\\nAnd mess up his face\\nIll even try to snatch both of\\nHis legs outta place\\nBAD bad\\nOne more question honey\\nBefore I start to stutter\\nI can even tell you why\\nWhite milk make yellow butter\\nIm BAD bad\\nYou ask me honey\\nWhat it was all about\\nYou even asked me where the light went\\nWhen it went out\\nIm BAD\\nIm bad\\nDont mess with me\\nAhh ahh ahh ahh\\nAhh ahh ahh ahh\\nWhahoo\\nWhahoo\\n', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<eos>', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 982}]\n"
     ]
    }
   ],
   "source": [
    "print(template.wrap_one_example(dataset['train'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-08T09:49:14.303841Z",
     "iopub.status.busy": "2022-12-08T09:49:14.303617Z"
    },
    "id": "rYY1dSQqJVZd",
    "outputId": "0a82fb3e-fe9b-45ff-8b5a-108ff1a006b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1223 > 1024). Running this sequence through the model will result in indexing errors\n",
      "tokenizing: 3079it [00:12, 281.32it/s]"
     ]
    }
   ],
   "source": [
    "train_dataloader = get_dataloader(\n",
    "    dataset[\"train\"],\n",
    "    template,\n",
    "    tokenizer,\n",
    "    WrapperClass,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "validation_dataloader = get_dataloader(\n",
    "    dataset[\"validation\"],\n",
    "    template,\n",
    "    tokenizer,\n",
    "    WrapperClass,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCfLCZ2DJVZd"
   },
   "outputs": [],
   "source": [
    "prompt_model = PromptForGeneration(plm=plm,template=template, freeze_plm=True,tokenizer=tokenizer)\n",
    "prompt_model = prompt_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qroY_zsxJVZd"
   },
   "outputs": [],
   "source": [
    "# Follow PrefixTuningÔºàhttps://github.com/XiangLi1999/PrefixTuning), we also fix the language model\n",
    "# only include the template's parameters in training.\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in template.named_parameters()\n",
    "            if (not any(nd in n for nd in no_decay)) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in template.named_parameters()\n",
    "            if any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QunPpG9lJVZd"
   },
   "outputs": [],
   "source": [
    "tot_step  = len(train_dataloader)*5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 0, tot_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwcDTEzcJVZe"
   },
   "outputs": [],
   "source": [
    "generation_arguments = {\n",
    "    \"max_length\": 512,\n",
    "    \"max_new_tokens\": None,\n",
    "    \"min_length\": 5,\n",
    "    \"temperature\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"num_beams\": 5,\n",
    "    \"bad_words_ids\": [[628], [198]]\n",
    "}\n",
    "\n",
    "def evaluate(prompt_model, dataloader):\n",
    "    generated_sentence = []\n",
    "    prompt_model.eval()\n",
    "\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        inputs = inputs.cuda()\n",
    "        _, output_sentence = prompt_model.generate(inputs, **generation_arguments)\n",
    "        generated_sentence.extend(output_sentence)\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q74LF7c_JVZe",
    "outputId": "02d97812-d750-4fb4-d28f-6f6b666fdc3f"
   },
   "outputs": [],
   "source": [
    "# training and generation.\n",
    "global_step = 0\n",
    "tot_loss = 0\n",
    "log_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    prompt_model.train()\n",
    "    for step, inputs in tqdm(enumerate(train_dataloader)):\n",
    "        global_step +=1\n",
    "        inputs = inputs.cuda()\n",
    "        loss = prompt_model(inputs)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(template.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if global_step %500 ==0:\n",
    "            print(\"Epoch {}, global_step {} average loss: {} lr: {}\".format(epoch, global_step, (tot_loss-log_loss)/500, scheduler.get_last_lr()[0]), flush=True)\n",
    "            log_loss = tot_loss\n",
    "    \n",
    "    print(\"Generating lyrics...\")\n",
    "    generated_sentence = evaluate(prompt_model, validation_dataloader)    \n",
    "    with open(base_path + \"generated_sentences.txt\",'w') as f:\n",
    "        for i in generated_sentence:\n",
    "            f.write(i+\"\\n\")\n",
    "        f.close()\n",
    "    \n",
    "generated_sentence = evaluate(prompt_model, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_65bC9ClJVZe"
   },
   "outputs": [],
   "source": [
    "with open(base_path + \"generated_sentences.txt\",'w') as f:\n",
    "    for i in [\"test\"]:\n",
    "        f.write(i+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
