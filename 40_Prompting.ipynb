{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "Notebook to perform prompting with OpenPrompt on the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the few resources found to Prefix Templates with OpenPrompt\n",
    "\n",
    "> https://github.com/thunlp/OpenPrompt/blob/main/tutorial/2.1_conditional_generation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q openprompt transformers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2022-12-08T12:25:32.251453Z",
     "shell.execute_reply": "2022-12-08T12:25:32.249993Z",
     "shell.execute_reply.started": "2022-12-08T12:25:27.034051Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt import PromptDataLoader, PromptForGeneration\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import PrefixTuningTemplate\n",
    "from openprompt.utils.metrics import generation_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets.dataset_dict import DatasetDict, Dataset\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Set paths and parametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:25:32.255720Z",
     "iopub.status.busy": "2022-12-08T12:25:32.255059Z",
     "iopub.status.idle": "2022-12-08T12:25:32.262487Z",
     "shell.execute_reply": "2022-12-08T12:25:32.261021Z",
     "shell.execute_reply.started": "2022-12-08T12:25:32.255720Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"./evaluations/\"\n",
    "csv_data = \"./df_songs.csv\"\n",
    "used_model = \"gpt2\"\n",
    "\n",
    "train_split = 0.7\n",
    "epochs = 10\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model / Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the CSV, remove everything except the lyrics. Add index for flavour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:25:32.270194Z",
     "iopub.status.busy": "2022-12-08T12:25:32.269979Z",
     "iopub.status.idle": "2022-12-08T12:25:32.529671Z"
    }
   },
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv(csv_data)\n",
    "lyrics_df = lyrics_df.drop(\n",
    "    columns=[\"Artist\", \"Song\", \"LyricsWordCount\"], errors=\"ignore\"\n",
    ").reset_index(level=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset and create an DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:25:32.534473Z",
     "iopub.status.busy": "2022-12-08T12:25:32.534209Z",
     "iopub.status.idle": "2022-12-08T12:25:32.586316Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(lyrics_df, train_size=train_split)\n",
    "train_dataset, validation_dataset = Dataset.from_pandas(train_df), Dataset.from_pandas(\n",
    "    validation_df\n",
    ")\n",
    "raw_dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataset with a mapped `InputExample` for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:25:32.588010Z",
     "iopub.status.busy": "2022-12-08T12:25:32.587529Z",
     "iopub.status.idle": "2022-12-08T12:25:33.316036Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    for data in raw_dataset[split]:\n",
    "        # input_example = InputExample(text_a = data['premise'], text_b = data['hypothesis'], label=int(data['label']), guid=data['idx'])\n",
    "        input_example = InputExample(text_a = data['Lyrics'], guid=data['index'])\n",
    "        dataset[split].append(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steal this dataloader wrapper function üê±‚Äçüë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:25:33.318136Z",
     "iopub.status.busy": "2022-12-08T12:25:33.317674Z",
     "iopub.status.idle": "2022-12-08T12:25:33.326988Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    dataset_split, template, tokenizer, wrapper_class, shuffle=False, batch_size=32\n",
    "):\n",
    "    \"\"\"Returns a prompt data load for a given dataset split and template\"\"\"\n",
    "\n",
    "    return PromptDataLoader(\n",
    "        dataset=dataset_split,\n",
    "        template=template,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=wrapper_class,\n",
    "        max_seq_length=256,\n",
    "        decoder_max_length=256,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        teacher_forcing=False,\n",
    "        predict_eos_token=True,\n",
    "        truncate_method=\"head\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model (PLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:25:33.329350Z",
     "iopub.status.busy": "2022-12-08T12:25:33.328738Z",
     "iopub.status.idle": "2022-12-08T12:25:37.044297Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm(used_model, used_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:25:37.049669Z",
     "iopub.status.busy": "2022-12-08T12:25:37.049205Z",
     "iopub.status.idle": "2022-12-08T12:25:37.055950Z"
    }
   },
   "outputs": [],
   "source": [
    "# # tokenizer = GPT2Tokenizer.from_pretrained(used_model, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "\n",
    "# tokenizer.bos_token = \"<|startoftext|>\"\n",
    "# tokenizer.eos_token = \"<|endoftext|>\"\n",
    "# tokenizer.pad_token = \"<|pad|>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prompt-Based Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a template.\n",
    "The used template (line 1) equals the last template (line 7), so that the text param can be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:25:37.057681Z",
     "iopub.status.busy": "2022-12-08T12:25:37.057320Z",
     "iopub.status.idle": "2022-12-08T12:25:37.262166Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignore using_decoder_past_key_values=False in a decoder-only LM.\n"
     ]
    }
   ],
   "source": [
    "template = PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text=' {\"placeholder\":\"text_a\"} {\"special\": \"<eos>\"} {\"mask\"} ', using_decoder_past_key_values=False)\n",
    "\n",
    "# Are the tokens necessary? Probably not:\n",
    "# # You may observe that the example doesn't end with <|endoftext|> token. Don't worry, adding specific end-of-text token\n",
    "# # is a language-model-specific token. we will add it for you in the TokenizerWrapper once you pass `predict_eos_token=True`\n",
    "\n",
    "# template = PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text='<|startoftext|>{\"placeholder\":\"text_a\"} {\"mask\"}<|endoftext|>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one example and print it, to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:25:37.264150Z",
     "iopub.status.busy": "2022-12-08T12:25:37.263226Z",
     "iopub.status.idle": "2022-12-08T12:25:37.269937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': ' Still Not A Player Clean Version Lyrics\\nDont wanna be a player TS \\nI dont wanna be a playa no more\\nIm not a playa I just crush a lot\\nBut Big Punisher still got what youre lookin for\\nFor my thugs I dont wanna be for my thugs I dont wanna be a playa no more \\nI dont wanna be a playa no more\\nIm not a playa I just crush a lot\\nBut you know Big Punisher still down by law\\nWhos down to crush a lot\\nHey yo Im still not a player but you still a hater\\nElevator to the top hah see you later Im gone\\nPenthouse suite Penthouse freaks\\nIn house beach French countess ten thou piece\\nRentout lease with a option to buy\\nCoppin a fiveoh Benz for when Im not far up in the sky\\nPuffin the lye from my Twinzito\\nUp in the Benzito with my kiko from Queens nicknamed Perico\\nWe go back like PAs and wearin PJs\\nNow we reach the peakage runnin trains for three days\\nWho wanna ride it wont cost you a dollar\\nWhether soft or harder of course you still gonna holla\\nMy my Im big huh I rip my through your hooters\\nIm sick you couldnt measure my with six rulers\\nHold up chula Im all about getting loot\\nBut I knock that boot if you out to get HOOF\\nI dont wanna be a playa no more\\nIm not a playa I just crush a lot No more \\nBut Big Punisher still got what youre lookin for\\nUptown baby uptown\\nI dont wanna be a playa no more\\nIm not a playa I just crush a lot I dont wanna be a playa \\nBut you know Big Punisher still down by law\\nWhos down to crush tonight\\nI love from butter pecan to blackberry molass\\nI dont discriminate I regulate every shade of the\\nLong as you show class and pass my test\\nFat and breasts highly intelligent bachlorettes\\nThats the best I wont settle for less\\nI wanna ghetto brunette with unforgettable sex\\nI lay your head on my chest come feel my heartbeat\\nWe can park the Jeep pump Mobb Deep and just spark the leaf\\nIts hard to creep since I found Joe\\nEvery pretty round brown wanna go down low\\nBut this Boogie Down professional Im let you know\\nOnce I put the blows get your clothes cause you got to go\\nI could go downstairs little brown hairs everywhere\\nYou nasty Twin I dont care\\nRound here they call me Big Pun if you with the big guns\\nThick tongue known to make a chick come \\nIn the hot tub poppin bubbly\\nRubbin your spot love got you screamin Punish me\\nBut it dont stop watch the Pun get wicked\\nWhen I even Luke be like Dont stop get it get it\\nIn the hot tub poppin bubbly\\nRubbin your spot love got you screamin Punish me\\nBut it dont stop watch the Pun get wicked\\nWhen I even Luke be like Dont stop get it get it\\nI dont wanna be a playa no more Punisher Punisher Punisher Big Punisher \\nBut Big Punisher still got what youre lookin for Punisher ooh Big Punisher \\nI dont wanna be a playa no more Punisher Punisher Punisher Big Punisher \\nBut you know Big Punisher still down by law Punisher ooh Big Punisher \\nWhos down to crush a lot\\nBoricua morena boricua morena\\nBoricua morena boricua morena\\nBoricua morena boricua morena\\nNo more rollin with an entourage\\nUnless its Pun and the Terror Squad\\nPunisher Punisher Punisher Big Punisher\\nPunisher Joe and Big Punisher\\nBoricua morena boricua morena\\nBoricua morena boricua morena\\nBoricua morena boricua morena\\nBoricua morena boricua morena\\nBoricua morena boricua morena\\nBoricua morena boricua morena\\n', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<eos>', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 6160}]\n"
     ]
    }
   ],
   "source": [
    "print(template.wrap_one_example(dataset['train'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:25:37.272356Z",
     "iopub.status.busy": "2022-12-08T12:25:37.271274Z",
     "iopub.status.idle": "2022-12-08T12:26:12.708095Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
      "tokenizing: 5714it [00:25, 225.69it/s]\n",
      "tokenizing: 2449it [00:09, 244.91it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = get_dataloader(\n",
    "    dataset[\"train\"],\n",
    "    template,\n",
    "    tokenizer,\n",
    "    WrapperClass,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "validation_dataloader = get_dataloader(\n",
    "    dataset[\"validation\"],\n",
    "    template,\n",
    "    tokenizer,\n",
    "    WrapperClass,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:26:12.709885Z",
     "iopub.status.busy": "2022-12-08T12:26:12.709529Z",
     "iopub.status.idle": "2022-12-08T12:26:15.315619Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_model = PromptForGeneration(plm=plm,template=template, freeze_plm=True,tokenizer=tokenizer)\n",
    "prompt_model = prompt_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:26:15.317034Z",
     "iopub.status.busy": "2022-12-08T12:26:15.316683Z",
     "iopub.status.idle": "2022-12-08T12:26:15.325120Z"
    }
   },
   "outputs": [],
   "source": [
    "# Follow PrefixTuningÔºàhttps://github.com/XiangLi1999/PrefixTuning), we also fix the language model\n",
    "# only include the template's parameters in training.\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in template.named_parameters()\n",
    "            if (not any(nd in n for nd in no_decay)) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in template.named_parameters()\n",
    "            if any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:26:15.326430Z",
     "iopub.status.busy": "2022-12-08T12:26:15.326190Z",
     "iopub.status.idle": "2022-12-08T12:26:15.340975Z"
    }
   },
   "outputs": [],
   "source": [
    "tot_step  = len(train_dataloader)*5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 0, tot_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T14:25:14.785800Z",
     "iopub.status.busy": "2022-12-08T14:25:14.785366Z",
     "iopub.status.idle": "2022-12-08T14:25:14.793264Z",
     "shell.execute_reply": "2022-12-08T14:25:14.791821Z",
     "shell.execute_reply.started": "2022-12-08T14:25:14.785772Z"
    }
   },
   "outputs": [],
   "source": [
    "generation_arguments = {\n",
    "    \"max_length\": 512,\n",
    "    \"max_new_tokens\": None,\n",
    "    \"min_length\": 5,\n",
    "    \"temperature\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"num_beams\": 5,\n",
    "    \"bad_words_ids\": [[628], [198]]\n",
    "}\n",
    "\n",
    "def evaluate(prompt_model, dataloader):\n",
    "    generated_sentence = []\n",
    "    prompt_model.eval()\n",
    "\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        inputs = inputs.cuda()\n",
    "        _, output_sentence = prompt_model.generate(inputs, **generation_arguments)\n",
    "        generated_sentence.extend(output_sentence)\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T12:26:15.350851Z",
     "iopub.status.busy": "2022-12-08T12:26:15.350610Z",
     "iopub.status.idle": "2022-12-08T14:05:46.017673Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "499it [06:59,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, global_step 500 average loss: 1.5598060819145378 lr: 2.5804195804195803e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [09:59,  1.19it/s]\n",
      "284it [03:58,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, global_step 1000 average loss: 4.750476747403809e-06 lr: 2.160839160839161e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [09:58,  1.19it/s]\n",
      "69it [00:57,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, global_step 1500 average loss: 1.2531490560832025e-06 lr: 1.7412587412587412e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "569it [07:56,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, global_step 2000 average loss: 0.0001111410981204699 lr: 1.3216783216783218e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [09:58,  1.20it/s]\n",
      "354it [04:55,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, global_step 2500 average loss: 1.649537864523154e-07 lr: 9.02097902097902e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [09:57,  1.20it/s]\n",
      "139it [01:56,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, global_step 3000 average loss: 9.813892097554344e-08 lr: 4.8251748251748255e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "639it [08:54,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, global_step 3500 average loss: 1.1652641319415125e-07 lr: 6.293706293706294e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [09:57,  1.20it/s]\n",
      "424it [05:53,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, global_step 4000 average loss: 1.026688223646488e-07 lr: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [09:55,  1.20it/s]\n",
      "209it [02:54,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, global_step 4500 average loss: 9.489052081335103e-08 lr: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "709it [09:51,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, global_step 5000 average loss: 1.0597574510029517e-07 lr: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [09:56,  1.20it/s]\n",
      "494it [06:51,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, global_step 5500 average loss: 9.727467613629414e-08 lr: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [09:54,  1.20it/s]\n",
      "279it [03:52,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, global_step 6000 average loss: 1.2564512326207478e-07 lr: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [09:55,  1.20it/s]\n",
      "64it [00:53,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, global_step 6500 average loss: 9.843688849286992e-08 lr: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "564it [07:49,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, global_step 7000 average loss: 1.2248712528162287e-07 lr: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "715it [09:54,  1.20it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, global_step \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m average loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m lr: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, global_step, (tot_loss\u001b[38;5;241m-\u001b[39mlog_loss)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m500\u001b[39m, scheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]), flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m             log_loss \u001b[38;5;241m=\u001b[39m tot_loss\n\u001b[0;32m---> 21\u001b[0m generated_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(prompt_model, dataloader)\u001b[0m\n\u001b[1;32m     21\u001b[0m     _, output_sentence \u001b[38;5;241m=\u001b[39m prompt_model\u001b[38;5;241m.\u001b[39mgenerate(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_arguments)\n\u001b[1;32m     22\u001b[0m     generated_sentence\u001b[38;5;241m.\u001b[39mextend(output_sentence)\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mgroundtruth_sentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtgt_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m score \u001b[38;5;241m=\u001b[39m generation_metric(generated_sentence, groundtruth_sentence, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_bleu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, score, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# training and generation.\n",
    "global_step = 0\n",
    "tot_loss = 0\n",
    "log_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    prompt_model.train()\n",
    "    for step, inputs in tqdm(enumerate(train_dataloader)):\n",
    "        inputs = inputs.cuda()\n",
    "        global_step +=1\n",
    "        loss = prompt_model(inputs)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(template.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if global_step %500 ==0:\n",
    "            print(\"Epoch {}, global_step {} average loss: {} lr: {}\".format(epoch, global_step, (tot_loss-log_loss)/500, scheduler.get_last_lr()[0]), flush=True)\n",
    "            log_loss = tot_loss\n",
    "\n",
    "generated_sentence = evaluate(prompt_model, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T14:25:35.318997Z",
     "iopub.status.busy": "2022-12-08T14:25:35.318646Z",
     "iopub.status.idle": "2022-12-08T14:36:41.465033Z",
     "shell.execute_reply": "2022-12-08T14:36:41.464026Z",
     "shell.execute_reply.started": "2022-12-08T14:25:35.318973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'WASHINGTON',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'WASHINGTON',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sentence = evaluate(prompt_model, validation_dataloader)\n",
    "generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T14:42:03.505174Z",
     "iopub.status.busy": "2022-12-08T14:42:03.504759Z",
     "iopub.status.idle": "2022-12-08T14:42:03.866811Z",
     "shell.execute_reply": "2022-12-08T14:42:03.864857Z",
     "shell.execute_reply.started": "2022-12-08T14:42:03.505173Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 4it [00:00, 896.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ShareShare', 'The', '', '']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = evaluate(prompt_model, get_dataloader(\n",
    "    [InputExample(text_a =text, guid=data['index']) for text in [\"Snow outside\", \"What is love\", \"Give me a song\", \"<|startoftext|>\"]],\n",
    "    template,\n",
    "    tokenizer,\n",
    "    WrapperClass,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "))\n",
    "test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-08T14:05:46.025414Z",
     "iopub.status.idle": "2022-12-08T14:05:46.025987Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(base_path + \"generated_sentences.txt\",'w') as f:\n",
    "    for i in generated_sentence:\n",
    "        f.write(i+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Nov 15 2022, 05:43:36) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
