{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the few resources found to Prefix Templates with OpenPrompt\n",
    "\n",
    "> https://github.com/thunlp/OpenPrompt/blob/main/tutorial/2.1_conditional_generation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q openprompt transformers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sebastian\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader, PromptForGeneration\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import PrefixTuningTemplate\n",
    "from openprompt.utils.metrics import generation_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets.dataset_dict import DatasetDict, Dataset\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "!mkdir -p \"/content/gdrive/My Drive/LyricsGenerator/\"\n",
    "base_path = Path(\"/content/gdrive/My Drive/LyricsGenerator/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = \"./datasets/df_songs.csv\"\n",
    "used_model = \"gpt2\"\n",
    "\n",
    "train_split = 0.7\n",
    "epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model / Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the CSV, remove everything except the lyrics. Add index for flavour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv(csv_data)\n",
    "lyrics_df = lyrics_df.drop(\n",
    "    columns=[\"Artist\", \"Song\", \"LyricsWordCount\"], errors=\"ignore\"\n",
    ").reset_index(level=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset and create an DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(lyrics_df, train_size=train_split)\n",
    "train_dataset, validation_dataset = Dataset.from_pandas(train_df), Dataset.from_pandas(\n",
    "    validation_df\n",
    ")\n",
    "raw_dataset = DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataset with a mapped `InputExample` for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "for split in ['train', 'validation']:\n",
    "    dataset[split] = []\n",
    "    for data in raw_dataset[split]:\n",
    "        # input_example = InputExample(text_a = data['premise'], text_b = data['hypothesis'], label=int(data['label']), guid=data['idx'])\n",
    "        input_example = InputExample(text_a = data['Lyrics'], guid=data['index'])\n",
    "        dataset[split].append(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steal this dataloader wrapper function üê±‚Äçüë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    dataset_split, template, tokenizer, wrapper_class, shuffle=False, batch_size=32\n",
    "):\n",
    "    \"\"\"Returns a prompt data load for a given dataset split and template\"\"\"\n",
    "\n",
    "    return PromptDataLoader(\n",
    "        dataset=dataset_split,\n",
    "        template=template,\n",
    "        tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=wrapper_class,\n",
    "        max_seq_length=256,\n",
    "        decoder_max_length=256,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        teacher_forcing=False,\n",
    "        predict_eos_token=True,\n",
    "        truncate_method=\"head\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model (PLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm(used_model, used_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer = GPT2Tokenizer.from_pretrained(used_model, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "\n",
    "# tokenizer.bos_token = \"<|startoftext|>\"\n",
    "# tokenizer.eos_token = \"<|endoftext|>\"\n",
    "# tokenizer.pad_token = \"<|pad|>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt-Based Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a template.\n",
    "The used template (line 1) equals the last template (line 7), so that the text param can be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignore using_decoder_past_key_values=False in a decoder-only LM.\n"
     ]
    }
   ],
   "source": [
    "template = PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text=' {\"placeholder\":\"text_a\"} {\"special\": \"<eos>\"} {\"mask\"} ', using_decoder_past_key_values=False)\n",
    "\n",
    "# Are the tokens necessary? Probably not:\n",
    "# # You may observe that the example doesn't end with <|endoftext|> token. Don't worry, adding specific end-of-text token\n",
    "# # is a language-model-specific token. we will add it for you in the TokenizerWrapper once you pass `predict_eos_token=True`\n",
    "\n",
    "# template = PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text='<|startoftext|>{\"placeholder\":\"text_a\"} {\"mask\"}<|endoftext|>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one example and print it, to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': ' Snoopy\\nIs what they say as if they knew me\\nGroupies on my head like a cufi\\nMy nigga Kid Cudi thats my little buddy\\nCall some hoes up and get some cutty cutty\\nWhats your life like Mines is kinda tight\\nA long way from hustlin that china white\\nMy people love me the fans love me Im all go\\nIf you aint showin love then what you call for\\nI dont need it in my life my kids pay the price\\nSo alike and in need of my wife\\nSmoke til Im gone Dogg on the throne\\nIm worldwide known Im a boss in the zone\\nI get it how I get it cause I can I earn my spot\\nYou see the plan Im just fuckin with my fans\\nI turned out to be the better man important to life\\nYou understand while Im puffin on this gram\\nEverything Im havin no there aint necessity\\nThough Im shining keep on grindin\\nWhat you see aint all of me\\nThough I keep them hos dont love them hos\\nThe code in which yall roll\\nIts so simple what I need\\nYou know I keep my fam and I cant forget that tree\\nNah nah nah nah nah nah\\nI cant forget that tree\\nNah nah nah nah nah nah\\nI cant forget that tree\\nNah nah nah nah nah nah\\nI cant forget that tree\\nNah nah nah nah nah nah\\nNah nah nah nah nah\\nNah nah nah nah nah nah \\nI got my fam dawg look at my niggas\\nGot a little money yall and I aint trippin\\nMost days Im faded feeling exrated\\nSteady wantin freaky hoes then I want a lady\\nHoldin the fam down and my close homie\\nThe peoples who were there when some sucka left me lonely\\nPhony when you see a nigga dont approach me\\nMade it to the top baby thats on me\\nKoch tryna claim that they made me\\nHowever you feel another real fugazzi\\nI keep it one hundred never frontin dont need be\\nIm in another zone no place you could see my nigga\\nConyo to haters that dont know\\nLow pro how a nigga like from costo\\nThe one fans love who was always approachable yo\\nAnd I rep the double O smokin eatin Cheerios\\nEverything Im havin no there aint necessity\\nThough Im shining keep on grindin\\nWhat you see aint all of me\\nThough I keep them hos dont love them hos\\nThe code in which yall roll\\nIts so simple what I need\\nYou know I keep my fam and I cant forget that tree\\nNah nah nah nah nah nah\\nI cant forget that tree\\nNah nah nah nah nah nah\\nI cant forget that tree\\nNah nah nah nah nah nah\\nI cant forget that tree\\nNah nah nah nah nah nah\\nNah nah nah nah nah\\nNah nah nah nah nah nah \\nCant forget it they made me who I am a king\\nAnd Im still on the scene you gotta love it\\nAll out in public the people want to touch it\\nI need a blunt with the stuffin\\nNigga its nothin for me to chill out post up at the house\\nWith Madden on the screen and smash on the homies\\nHollywood nice now everybody on me\\nAnd on the blogs talk shit like they know me hm\\nAww fool take you niggas to school\\nHow to get rich and stay cool\\nSnoopy D O Dub show you homie even with the critics\\nAnd the Feds all on me I was tappin all the chonies\\nDogg but my folks call me Snoopy\\nBoss dogg nigga sue me\\nPony tail swangin as I lay in the jacuzzi\\nLove of my life while Im puffin on the oww wee kush\\nEverything Im havin no there aint necessity\\nThough Im shining keep on grindin\\nWhat you see aint all of me\\nThough I keep them hos dont love them hos\\nThe code in which yall roll\\nIts so simple what I need\\nYou know I keep my fam and I cant forget that tree\\nNah nah nah nah nah nah\\nI cant forget that tree\\nNah nah nah nah nah nah\\nI cant forget that tree\\nNah nah nah nah nah nah\\nI cant forget that tree\\nNah nah nah nah nah nah\\nNah nah nah nah nah\\nSnoop Dogg in the house tonight blowing that Laker purp that I know you like right\\nKid Cudi my nephew blaze that shit right now would you would you right\\nSnoop Dogg in the house tonight blazing that Laker purp that I know you like right\\nKid Cudi my nephew blaze that shit right now my nigga right\\n', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<eos>', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 7311}]\n"
     ]
    }
   ],
   "source": [
    "print(template.wrap_one_example(dataset['train'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors\n",
      "tokenizing: 5714it [00:13, 431.26it/s]\n",
      "tokenizing: 2449it [00:05, 466.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = get_dataloader(\n",
    "    dataset[\"train\"],\n",
    "    template,\n",
    "    tokenizer,\n",
    "    WrapperClass,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "validation_dataloader = get_dataloader(\n",
    "    dataset[\"validation\"],\n",
    "    template,\n",
    "    tokenizer,\n",
    "    WrapperClass,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_model = PromptForGeneration(plm=plm,template=template, freeze_plm=True,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow PrefixTuningÔºàhttps://github.com/XiangLi1999/PrefixTuning), we also fix the language model\n",
    "# only include the template's parameters in training.\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in template.named_parameters()\n",
    "            if (not any(nd in n for nd in no_decay)) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in template.named_parameters()\n",
    "            if any(nd in n for nd in no_decay) and p.requires_grad\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_step  = len(train_dataloader)*5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 0, tot_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_arguments = {\n",
    "    \"max_length\": 512,\n",
    "    \"max_new_tokens\": None,\n",
    "    \"min_length\": 5,\n",
    "    \"temperature\": 1.0,\n",
    "    \"do_sample\": False,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"num_beams\": 5,\n",
    "    \"bad_words_ids\": [[628], [198]]\n",
    "}\n",
    "\n",
    "def evaluate(prompt_model, dataloader):\n",
    "    generated_sentence = []\n",
    "    groundtruth_sentence = []\n",
    "    prompt_model.eval()\n",
    "\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        _, output_sentence = prompt_model.generate(inputs, **generation_arguments)\n",
    "        generated_sentence.extend(output_sentence)\n",
    "        groundtruth_sentence.extend(inputs['tgt_text'])\n",
    "    score = generation_metric(generated_sentence, groundtruth_sentence, \"sentence_bleu\")\n",
    "    print(\"test_score\", score, flush=True)\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [10:41, 42.80s/it]"
     ]
    }
   ],
   "source": [
    "# training and generation.\n",
    "global_step = 0\n",
    "tot_loss = 0\n",
    "log_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    prompt_model.train()\n",
    "    for step, inputs in tqdm(enumerate(train_dataloader)):\n",
    "        global_step +=1\n",
    "        loss = prompt_model(inputs)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(template.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if global_step %500 ==0:\n",
    "            print(\"Epoch {}, global_step {} average loss: {} lr: {}\".format(epoch, global_step, (tot_loss-log_loss)/500, scheduler.get_last_lr()[0]), flush=True)\n",
    "            log_loss = tot_loss\n",
    "\n",
    "generated_sentence = evaluate(prompt_model, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + \"generated_sentences.txt\",'w') as f:\n",
    "    for i in generated_sentence:\n",
    "        f.write(i+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "487b39d4bc77932302fbf00c8aa33c8cae154b5482e37c69cf95409c8a1ceaae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
